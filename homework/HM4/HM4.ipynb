{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nakX8slKh4kW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baLiRIYnfFAU"
   },
   "source": [
    "# Home 4: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Ian Gomez]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0qomDeXfFAX"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEB9zeQqfFAZ"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InpMQsEjfFAa"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKzoCUurfFAa",
    "outputId": "4faf286e-67ba-4a71-eb03-d4794867c257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "#Normalize Dataset\n",
    "x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvW8oZjXfFAb"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_ykQvxEfFAc",
    "outputId": "d2ed7093-c72a-4ed8-d719-b94fc856860e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    return np.array(tf.one_hot(y.reshape(-1), num_class))\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9B4ukN_fFAc"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaKNfgXUfFAd"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEP9vHdTfFAd",
    "outputId": "be76ad29-ecdc-484b-aed8-343cb9daa141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = np.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY1z5TZffFAe"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl8MJNVZfFAe"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KNRjH4vfFAe",
    "outputId": "bae87d93-05a2-441b-882a-cba7605417b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 49,691,466\n",
      "Trainable params: 49,661,514\n",
      "Non-trainable params: 29,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "import keras.activations as activations\n",
    "\n",
    "#Using VGG-16 Architecture while using dropout and other batch normalization\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0HtsjVcfFAf"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5Ue-nGPfFAf",
    "outputId": "d908cf63-8f08-494d-99f2-d4d5aaab525b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1250/1250 [==============================] - 37s 27ms/step - loss: 2.4142 - acc: 0.1806 - val_loss: 4.4136 - val_acc: 0.1723\n",
      "Epoch 2/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.7118 - acc: 0.3788 - val_loss: 2.4917 - val_acc: 0.2711\n",
      "Epoch 3/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.4980 - acc: 0.4603 - val_loss: 2.4447 - val_acc: 0.3165\n",
      "Epoch 4/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.3715 - acc: 0.5103 - val_loss: 2.7354 - val_acc: 0.3147\n",
      "Epoch 5/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.2682 - acc: 0.5505 - val_loss: 1.5356 - val_acc: 0.5052\n",
      "Epoch 6/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.2256 - acc: 0.5803 - val_loss: 2.0315 - val_acc: 0.3719\n",
      "Epoch 7/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.1604 - acc: 0.6037 - val_loss: 1.3674 - val_acc: 0.5452\n",
      "Epoch 8/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.1058 - acc: 0.6257 - val_loss: 1.2745 - val_acc: 0.6030\n",
      "Epoch 9/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.0642 - acc: 0.6439 - val_loss: 1.1387 - val_acc: 0.6241\n",
      "Epoch 10/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.0139 - acc: 0.6602 - val_loss: 1.1230 - val_acc: 0.6380\n",
      "Epoch 11/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.9566 - acc: 0.6783 - val_loss: 1.1918 - val_acc: 0.6228\n",
      "Epoch 12/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.9339 - acc: 0.6863 - val_loss: 1.1754 - val_acc: 0.6290\n",
      "Epoch 13/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.9115 - acc: 0.7032 - val_loss: 0.9647 - val_acc: 0.6823\n",
      "Epoch 14/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.8550 - acc: 0.7125 - val_loss: 0.8369 - val_acc: 0.7131\n",
      "Epoch 15/50\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.8409 - acc: 0.7164 - val_loss: 0.7463 - val_acc: 0.7417\n",
      "Epoch 16/50\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.8379 - acc: 0.7297 - val_loss: 0.8332 - val_acc: 0.7123\n",
      "Epoch 17/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7919 - acc: 0.7326 - val_loss: 0.8755 - val_acc: 0.7218\n",
      "Epoch 18/50\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.8442 - acc: 0.7403 - val_loss: 0.7759 - val_acc: 0.7480\n",
      "Epoch 19/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7580 - acc: 0.7489 - val_loss: 0.7863 - val_acc: 0.7409\n",
      "Epoch 20/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7417 - acc: 0.7546 - val_loss: 0.8164 - val_acc: 0.7523\n",
      "Epoch 21/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7069 - acc: 0.7663 - val_loss: 0.7122 - val_acc: 0.7597\n",
      "Epoch 22/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7103 - acc: 0.7682 - val_loss: 0.9894 - val_acc: 0.7074\n",
      "Epoch 23/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6819 - acc: 0.7714 - val_loss: 0.6967 - val_acc: 0.7710\n",
      "Epoch 24/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6615 - acc: 0.7846 - val_loss: 0.7106 - val_acc: 0.7613\n",
      "Epoch 25/50\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.6713 - acc: 0.7832 - val_loss: 0.6687 - val_acc: 0.7741\n",
      "Epoch 26/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6367 - acc: 0.7851 - val_loss: 0.6284 - val_acc: 0.7932\n",
      "Epoch 27/50\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.6230 - acc: 0.7945 - val_loss: 0.6372 - val_acc: 0.7943\n",
      "Epoch 28/50\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.6191 - acc: 0.7921 - val_loss: 0.6122 - val_acc: 0.8106\n",
      "Epoch 29/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6067 - acc: 0.7963 - val_loss: 0.6135 - val_acc: 0.8024\n",
      "Epoch 30/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6204 - acc: 0.7989 - val_loss: 0.6287 - val_acc: 0.7945\n",
      "Epoch 31/50\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5736 - acc: 0.8066 - val_loss: 0.5603 - val_acc: 0.8188\n",
      "Epoch 32/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5633 - acc: 0.8127 - val_loss: 0.9177 - val_acc: 0.7338\n",
      "Epoch 33/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5672 - acc: 0.8131 - val_loss: 0.5342 - val_acc: 0.8255\n",
      "Epoch 34/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5614 - acc: 0.8152 - val_loss: 0.5727 - val_acc: 0.8121\n",
      "Epoch 35/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5345 - acc: 0.8194 - val_loss: 0.5922 - val_acc: 0.8075\n",
      "Epoch 36/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5230 - acc: 0.8209 - val_loss: 0.5586 - val_acc: 0.8169\n",
      "Epoch 37/50\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5115 - acc: 0.8285 - val_loss: 0.5608 - val_acc: 0.8096\n",
      "Epoch 38/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5484 - acc: 0.8299 - val_loss: 0.5379 - val_acc: 0.8277\n",
      "Epoch 39/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5064 - acc: 0.8309 - val_loss: 0.5214 - val_acc: 0.8326\n",
      "Epoch 40/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5032 - acc: 0.8317 - val_loss: 0.5861 - val_acc: 0.8164\n",
      "Epoch 41/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4936 - acc: 0.8359 - val_loss: 0.5393 - val_acc: 0.8231\n",
      "Epoch 42/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4838 - acc: 0.8416 - val_loss: 0.5409 - val_acc: 0.8242\n",
      "Epoch 43/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4948 - acc: 0.8438 - val_loss: 0.5658 - val_acc: 0.8252\n",
      "Epoch 44/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4733 - acc: 0.8492 - val_loss: 0.5295 - val_acc: 0.8268\n",
      "Epoch 45/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4603 - acc: 0.8464 - val_loss: 0.5474 - val_acc: 0.8348\n",
      "Epoch 46/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4391 - acc: 0.8534 - val_loss: 0.4995 - val_acc: 0.8379\n",
      "Epoch 47/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4442 - acc: 0.8506 - val_loss: 0.5492 - val_acc: 0.8284\n",
      "Epoch 48/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4390 - acc: 0.8561 - val_loss: 0.5308 - val_acc: 0.8367\n",
      "Epoch 49/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4478 - acc: 0.8543 - val_loss: 0.5109 - val_acc: 0.8465\n",
      "Epoch 50/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4130 - acc: 0.8618 - val_loss: 0.6289 - val_acc: 0.8137\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=32, epochs=50, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "6NKMCLsbfFAg",
    "outputId": "f27162de-346a-4087-8366-f7fbdf48f1fd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5fXA8e8hrGETENlCEkQQkC0QUUA2rRZFoaIoGC0gimjdqhaxtrS1pUWxLihWkCoIlEVRflBRVAruKHFBWQQjBBIUBYSArFnO7487EyaTmWSy3Mwkcz7PM8/MvXPnzrlZ7rnvct9XVBVjjDHRq1q4AzDGGBNelgiMMSbKWSIwxpgoZ4nAGGOinCUCY4yJctXDHUBJnX766ZqYmBjuMIwxplL59NNP96lq00DvVbpEkJiYSGpqarjDMMaYSkVEdgZ7z6qGjDEmylkiMMaYKGeJwBhjolylayMIJDs7m8zMTI4fPx7uUEwQtWvXJi4ujho1aoQ7FGOMnyqRCDIzM6lfvz6JiYmISLjDMX5Ulf3795OZmUmbNm3CHY4xxk+VqBo6fvw4TZo0sSQQoUSEJk2aWInNmFJasAASE6FaNed5wYLy3X+VSASAJYEIZ78fYxzBTupFrR8/HnbuBFXnefz48k0GVaJqyBhjIs2CBfDgg7BrF8THw5Qpzvrx4+HoUee196T+wQcwd27h9eDsw7ve6+hRZ31KSvnEWmVKBOG0f/9+unfvTvfu3WnevDmtWrXKXz558mSRn01NTeXOO+8s9jv69OlTXuEaY0qhJFfywa7i77or8El91qzgJ/tduwLHE2x9qahqpXr07NlT/W3evLnQuqLMn6+akKAq4jzPn1+ijxfpT3/6k06bNq3Auuzs7PL7gkqspL8nYypasHPD/PmqsbGqzmndecTGqt56a+D1TZoUXFfahzeOQO8lJJTs2IBUDXJejboSQUXUtwGMGTOGCRMmcN555zFx4kQ++eQTevfuTVJSEn369GHr1q0ArF27lssvvxyAP//5z9x4440MHDiQM888k+nTp+fvr169evnbDxw4kKuvvpoOHTqQkpKCemaZW7lyJR06dKBnz57ceeed+fv1lZ6eTr9+/ejRowc9evTgww8/zH/v4YcfpkuXLnTr1o1JkyYBkJaWxi9+8Qu6detGjx49+Pbbb8v3B2VMGJTkKt5bxVOSK/n9+0sWT0zMqddCXv5rb5VSbGzB7WNjT1U1lYtgGSJSH2UtEZRXdg3GWyIYPXq0DhkyRHNyclRVNSsrK79k8NZbb+nw4cNVVXXNmjU6ZMiQ/M/27t1bjx8/rnv37tXGjRvryZMnVVW1bt26+ds3aNBAMzIyNDc3V88//3x977339NixYxoXF6fbt29XVdWRI0fm79fXkSNH9NixY6qqum3bNvX+PFeuXKm9e/fWI0eOqKrq/v37VVW1V69e+sorr6iq6rFjx/LfLw0rEZiKFugKP9jVfbCreO/ny+MKv0mTICWLCXl6Sa21uoIheoQ6ej4famxswRJJWWsxKKJEEHWNxRVS3+YxYsQIYjypPisri9GjR/PNN98gImRnZwf8zJAhQ6hVqxa1atXijDPO4IcffiAuLq7ANr169cpf1717d9LT06lXrx5nnnlmfj/9UaNGMWvWrEL7z87O5vbbb+eLL74gJiaGbdu2AfD2228zduxYYj2XHo0bN+bw4cPs3r2bK6+8EnBuCjMmEpWkYbZOncBX8f7rvLz73BlgyLaYGMjNLby+SRM4dqzgPmNj4cknndfeWNu0zmHu0KVcsO5ROJHKvmpNOZTXgPnVx/LJ018wKsX5n0tJKb+G4UCirmooPr5k68uibt26+a//+Mc/MmjQIDZu3MiKFSuC9qmvVatW/uuYmBhycnJKtU0wjz/+OM2aNWPDhg2kpqYW25htjFtK2o0y2HslbZgtabVNoOoZIY92tTN49LL/cXuNmfyDSVzLIqqTnX/CnzULEhJAxHmeNevUCT39q8PkPTGdb6u144KnR8KhQzBzJqf/vJPmb86jbc5WRm37S0l/pKUWdSWCKVMKXiWAC/VtAWRlZdGqVSsA5syZU+77P/vss9m+fTvp6ekkJiayePHioHHExcVRrVo15s6dS67ncubiiy/moYceIiUlhdjYWH766ScaN25MXFwcy5Yt41e/+hUnTpwgNzc3v9RgTGl5T95FdaMU8ti5s1p+N0oonyv8YIJdxU+ZAilXHSf+4xVk/fslEo9u5iz5ltrHj8MKZ7tcqhFDHt/FxPHDFbeTdOnN0Lhxwat4VXjvfXj+eViyxPmiCy6AJ56AK65wshvAxRfDuHEwbRpcdRUkJ5fsQEoh6koEKSnBM7WbJk6cyAMPPEBSUlKJruBDVadOHZ555hkGDx5Mz549qV+/Pg0bNiy03W233cbcuXPp1q0bX3/9dX6pZfDgwQwdOpTk5GS6d+/Oo48+CsC8efOYPn06Xbt2pU+fPuzZs6fcYzdVW6CreP/G1+pk0/FoKjWfnc7zR69lF605QCOuZ15+N8pgDbYlvcJv0iRw42uhq/h4ZenEdaS8fyu0aEG/p67h8tM+oPPQttS+5zfw7LPw9tuwcycx2SdgxQpaDmhP0uJJ0Lo13HYbbN0K338PU6fC2WdD//6wdKlzwvn4Y3jvPRg27FQS8Hr0UWjWDMaOhYootQdrPIjUR3l0H62qDh8+rKqqeXl5euutt+pjjz0W5ogKst9T1VaShlnv66tZomsYoEeok79yJ611Idfqe/RVBZ3NjRrLkRI32AZrmPXG5Y31rPgT+vJT36l+9ZXq2rWqS5eq/vWvqu3bOx+qU0c1JUX1zTdVPZ0/irRhg+rYsao1azqfj4lxnvv1U50zR/Xnn0P7ga5Y4XzuT38q/S/FB0U0Fof9xF7ShyWC4B577DHt1q2bduzYUa+77roy9fBxg/2eItR996l266a6Zk2xm5a0n32wnjjVq+XqX3lQFXQTHfVx7tIRLNaEarvyt4khO3+bLTU664UttwQ94Tesc0IH8j9NYZ5O4Bl9sMbD+uWwP+iWX96li+reqIu4RlfXuUz3dOiv2qOH6tlnq7ZqpVqvXvBM0r+/6r//rZqVVbqf6549TkL5wx9Ut24t3T6uv161enXVL74o3ed9FJUIxHm/8khOTlb/qSq3bNlCx44dwxSRCZX9nipITg5UD635b8F85cIxrWmRuxuA7Rf8mjNfeZQFbzYtthcOOFUqs2Y51TaBetUEUoejzGU0I3iZ57iJ3zCDbGoSGwujRxccagFgaK1VLK55PTEnj3GLzuSFk049bgOy+FXN1/lLj/+j5YaV1Dx2qOAXiUC9elC/vvOoV6/wo359p67I/9GyJZxxRmgH5Kb9++Gcc5x4Pv4YyjCMu4h8qqqBGxyCZYhIfViJoPKy31MFWLtWtW5dVc+9H17Bqm061N6hCnov03QKD+hJquvhWo11Qs3ZKuSWqJ99C3br3Tyml7Nca3I84LYtydT19NRcRFOv+6cmxOcFLF0UKnVkZjpVK6ArYq/R1/mlnqCGs9OmTVXHjVNdvty58v7uO9XDh1Xz8irwB++ipUud4/z738u0G6xqyEQC+z25bONG1dNOc/6tR4/OX11UtU0K81RBu7BBQbUTG/U9LlAFfYd+nvV5RdbF9+JjfTX2Oj1J9fyVB2ioLzBar6m/UhvWOaGg2oNUzaSlHqKerrl3RcmPLztb9fe/91Tsn+VUab3/fmj19pXdiBFOm8OmTaXehSUCExHs9xREdrbq1Kmqd9+tmppauivZzEzVuDg9cloL/aRWX91BQv7VdLC76UH1X9yiB2io1cjJXyfk6o3M1v00UgX9jua6jKH6AFP0It7SBhzUGLJ1BIv1A3qrgp6sXV+fqn63tudrHcxKfYHReoCGqqDH6zbSZbGj9Ah1NCMmXv/79w1l+3kdO1Z1rvZDtWePasuWqi++WOpdhC0RAIOBrUAaMCnA+/HAGuBz4EvgsuL2aYmg8rLfUwA+VR5aw1PV0aWL6uOPq/74Y8CP+FedLJ51ULVLFz1Zu76eX/tzvZ3pqqCJbC9UEvB/fMU5+hqXFljn7eRyOj/qbTytc/i1buHsAh/0nuTTpK2uv+FJ1aysQnH954XjTnVNSopq/frOce7ZU4E/3Crm6NEyfTwsiQCIAb4FzgRqAhuATn7bzAJu9bzuBKQXt99ITAQDBw7UN954o8C6xx9/XCdMmBD0MwMGDND169erquqll16qBw4cKLRNoJFM/b366qu6yae4+Mc//lHfeuutkoRfYcL9e4o4q1Y59dt16zpn9wMHVP/1L9VevVRBc2Jq6Ot1rtQrWK5t408G7I5Zk+O6ttogzY2prilnvKmgeg5fqYKO5d8FTuz+j7Ma7VMFfYApBaqMgo2ouXjmAX37/jd1WsO/6vOM0XFN/08XvBhitUxurrs/S1OscCWC3sAqn+UHgAf8tpkJ3O+z/YfF7TcSE8HMmTN1zJgxBdadd955+s477wT9jG8iCCaURDB69Gh96aWXQg82jML9e4oYOTmqkyc7l87nnKPLH9lSqHF0xdSN+kT1e/UHmqqC/kBTnVH9Tr2wYWp+nb2QqwsYpQp6d5MXffrZ5+kPNNUXub7Aidz/xL7mnuWqoFc3eze0BltTqYUrEVwNzPZZvgF42m+bFsBXQCZwAOgZZF/jgVQgNT4+vtABhvsEs3//fm3atKmeOHFCVVV37NihrVu31ry8PJ0wYYL27NlTO3XqpJMnT87/jG8iSEhI0L1796qq6t/+9jdt166d9u3bV0eOHJmfCGbNmqXJycnatWtXHT58uB45ckQ/+OADbdSokSYmJmq3bt00LS2tQGJ4++23tXv37tq5c2cdO3asHj9+PP/7Jk+erElJSdq5c2fdsmVLoWPasWOHXnDBBZqUlKRJSUn6wQcf5L83depU7dy5s3bt2lXvv/9+VVX95ptv9KKLLtKuXbtqUlKSpqWlFdpnuH9P4TZ/vmpy3Pf6Nheqgqb1G6MLZ/9cZP/76pzUy1muS7haj+PcoLSRTno//9AnuUMVdBJ/LzRu/WJGaAatFPIKtBUUOLFPnOg0QHpGozVVWyQngnuAe/VUiWAzUK2o/RZbIrjrLtUBA8r3cdddxf6QhwwZosuWLVNV1X/84x967733quqp4ZxzcnJ0wIABumGD01AWKBGkpqZq586d9ciRI5qVlaVt27bNTwT79u3L/64HH3xQp0+frqqFSwTeZe+w1Fs9N7LccMMN+vjjj+d/n/fzM2bM0HHjxhU6HjeGq46WRPDKk7v01djrdC399auaSZrV7Cw92rCZ/oxzxj9CHR3D8yWewOQ0ftLxPJt/x62CzuDWAid7b1KZwDOqoF1qbwt+Nd+nj/MwUaGoRODmWEO7gdY+y3Gedb7GAUsAVPUjoDZwuosxuWbUqFEsWrQIgEWLFjFq1CgAlixZQo8ePUhKSmLTpk1s3rw56D7ee+89rrzySmJjY2nQoAFDhw7Nf2/jxo3069ePLl26sGDBAjZt2lRkPFu3bqVNmza0b98egNGjR/Puu+/mvz98+HAAevbsSXp6eqHPZ2dnc/PNN9OlSxdGjBiRH3eow1VX9YHpgo2Q+drUr+h1d28uOrocgPSTLVm1L5mXjg/lX9zKZP5CMqnMYWyJx8k5SCOWNrmFX8a+z1l8w0gWcgdPERsrzsBoPuNorWUQAM9dtybwOFrHjsH69c6gZybquTn66HqgnYi0wUkAI4Hr/LbZBVwEzBGRjjiJYG+ZvvWJJ8r08dIaNmwYv/3tb/nss884evQoPXv2ZMeOHTz66KOsX7+eRo0aMWbMmKDDTxdnzJgxLFu2jG7dujFnzhzWrl1bpni9Q1kHG8bad7jqvLy8qJ2LoCTj3DfbvIZ+//gVh7UeffmAr+jqbJDreZRA8ePZn8WSXWflx+Q92eePW69nQ8vmnHd0DU7Nqp/16yE72xKBAVwcfVRVc4DbgVXAFmCJqm4SkYdExHupey9ws4hsABYCYzxFmEqnXr16DBo0iBtvvDG/NHDo0CHq1q1Lw4YN+eGHH3j99deL3Ef//v1ZtmwZx44d4/Dhw6xYsSL/vcOHD9OiRQuys7NZ4DNAe/369Tl8+HChfZ199tmkp6eTlpYGOKOIDhgwIOTjycrKokWLFlSrVo158+YVGK76hRde4KjnDPXTTz9Rv379/OGqAU6cOJH/fmVWknHuhx5dSP+//5IMjaM3H51KAsUIeSTMBL/x7NMhL895DnjFLwKDBsGaNU7w/t5/33nu2zekOE3V5uow1Kq6UlXbq2pbVZ3iWTdZVZd7Xm9W1b6q2k1Vu6vqm27G47ZRo0axYcOG/ETQrVs3kpKS6NChA9dddx19i/mn69GjB9deey3dunXj0ksv5dxzz81/769//SvnnXceffv2pUOHDvnrR44cybRp00hKSiown3Dt2rV54YUXGDFiBF26dKFatWpMmDAh5GOJtuGqQxkqGQINe6zcxzQWch0f0odRce+TQeFZjlw74Rdl0CD44Qf4+uvC773/vjOGTePGJdypqZKCNR5E6iMSu4+a0ETq76m4oZKDPaqRk99zZxHXaLv440H35T/0cYV0yfzmGyeAZ54puD4nR7VhQ9VbbnE5ABNJCFNjsTERp6gr/9bsIpEdgLPsmW66kCZNYGCtj/iEXtzJU/yTexhXZyF/+nutIic+KvMVfkm1bQtxcU71kK+NGyEry9oHTD5LBKZKKsncts7wycpqLuJb2vIKV9KX98nN1ULVOYl1fuDjTmNZc6IPrWL2MJJFPJXwT2Y+V61Ag22FnvCD8bYTrF1bsJ3A2z5gicB4VJlEoJWzjTlqVOTvp6STmcfEQDKptCONNxhMf97lffrxWc3evDHuJc6Mz6E6Ofy50ZNspT1t1y2A+++n+cGtLNJrw3uyL86gQbB3L/h2N37/faekkJAQvrhMRKkSiaB27drs37/fkkGEUlX2799f7l1Qg/XlP9XIe+rvoag++7m5cH31xZykBiksIJ5d3F1jBm0b7affU9fwbbV2ZHfqxp8O3E3N/r3hq6+cOWjr1SvX43HFIOd+gvzqIVVnntwLLnBKDMbg7n0EFSYuLo7MzEz27i3bLQjGPbVr1yYuLq5Uny1JX/5ah/cxeOdSrmURffiQgaxlHb2L3H9ifB43HVnCu0d/SdbxRsTHw7lTbqPByFtgxQr45z/hwAFYtgyGDq1cJ9DEROexZg3ccYfzg9q926qFTEHBWpEj9RGo15CpukKZC7chB3Q0L+hKBms2zlCbm+mgB2mgi7gmf7tgk5m/MfkDZ6EMY71HtLFjVRs3dkYAnedMRFMec+CaygXrNWQqq6L68rfgO/7FBH6gGXMYy9lsZRoTeW3KFyTX2cxsbmI4r9Cc74vss//LrCVQqxYMGxaeg3TboEHw00/w5ZdO+0CDBtC5c7ijMhGkSlQNmaphwXxl9x3/IPfgz7zf/Gqum5bErl2Fq2Ea8RP38zB3Mp0YcnmeG3meG1nPuSQkCOm/h1kJ8NzECdz73WPc13A2zWf8sUCvnnx5eTDxJbj0UucEWRUNHOg8r1njJIK+fYP3jTXRKVhRIVIfVjVUNc2frzqxxmMF6m2+lTP1ydoTNZlPFPK0Lof19/xND9BQcxFdEHODJrK90I1bBVxyiWpcnDMdZCDvvON8eOFC148xrNq2Ve3rGbV0ypRwR2PCAKsaMq5auBAuvxwCjHkUSKDePqvuWcXfs+9jKcNpyo/cxHNs03bcevwx1tOLHbQhjbOYwh94P2YAr/9jAzr3RTShTaEbtwq49VbIzIT//jdwMEuWQJ06TvxV2aBB8MEHzmtrKDb+gmWISH1YiSDCfP65aq1azpXmDTcUu3mgxt9utb/WAzTUL+iqdTlc4L3G7NcPx7+gb9cZov/lMr2y+YclG5ohO9spEVxySeH3cnJUmzVTvfrqEuywklqwwPmB1qhR5rlvTeVEuCavd+NhiSCCZGWpnnWWasuWqnff7fw5zZ2rqsHH1fGdRcs72crXtNcfaKrxpBcazychoRzifOghZ2fbthVcv3q1s76STPVZJt995xxr797hjsSESVGJwKqGTOmows03k7d9ByNyF1H9iUdZV2sA2eNvY/m0rQHv7F2wwLkXwCuGHBYyijbsYDivsC+24J2usbGn7hkok5tugurV4dlnC65fvBjq1oXLLiuHL4lwLVrAyJEwdmy4IzGRKFiGiNSHlQgixIwZqqB/qDE1/+q9FRm6lya6Iaa71uR4wKt73xLBP/mtKug4ngs+r255ueYa1UaNTlWLZGc7NxaMGlWOX2JM5MKqhky5Sk1VrVlTV9e5TIXcAif7IaxQBX2COwslApFTbQRj+Xf+dgF7+5S3NWucIF54wVletcpZfvVVl7/YmMhQVCKwqiFTMllZcM010KwZ1xx7Ef8/ode4nMe5m7uYzhUsL/BefGslJe4dvu74K2ZzE2/xC6bH/zNwb5/yNmAAdOwI//qXs7x4sXPfwODBLn+xMZHPEoEJyYIFkJigLD3tRrK372LVjYupl9Ak4LbTGk/lC0niBcYSRwY1OMm4mvP4THrCwIG0Tn+fag/+nouzlvLtzuoVM3KniNOV9JNP4KOP4JVXnDuJo3QuZmN8WSIwxfIO63zFrqe5ileYxFSGT+vNZZcFnn5x2vRa7Jq2mFpykjcYTGZMArNP/prGdU84nf0zMuBvf6v4O3l//WsnwNGj4eBBuPbaiv1+YyJVsDqjSH1YG4G7AjXYJiSonsNXeoxaupzLFfLyG3+LbOCdP1+1WjXVwYNV33hDNS8vDEfkZ/x4p23gtNNUT5wIdzTGVBiKaCMQ5313iMhg4EkgBpitqlP93n8c8AyYTixwhqqeVtQ+k5OTNTU11Y1wo573yt93kLfYWMg5eoKPOY8WfE8XvmIvZwBObUteXjE7PXq0cLEhnL74ApKSnG6Uzz8f7miMqTAi8qmqJgd6z7VB50QkBpgBXAxkAutFZLmqbvZuo6q/9dn+DiDJrXhM8YKN9DlN/kh33cAQ/pufBMCZG6BYkZQEALp3h/nzoX//cEdiTMRws42gF5CmqttV9SSwCChqnN9RwEIX4zHF8L3Zy6s/73CPPsrs6rewkiH568vtZq9wSEmB1q3DHYUxEcPNRNAKyPBZzvSsK0REEoA2wP+CvD9eRFJFJNVmISsfgQZ+87/Cb0AWL/Jr0qufRb1n/1loHP+InafXGFMikdJraCTwsqrmBnpTVWeparKqJjdt2rSCQ6t6gk3u7t8L6GlupxW72fbH+YwcV5f0dKdNIKInazfGlJibiWA34Fv+jvOsC2QkVi1U7oqf3P2Uo0dh5cpTM3hdy2JuYD6brprM4Mm9Kjp0Y0wFcq3XkIhUB7YBF+EkgPXAdaq6yW+7DsAbQBsNIRjrNRSaYD2AZs2CG25wSgL+8nsBZWZCly7QoQO8954zYJsxplIrqteQayUCVc0BbgdWAVuAJaq6SUQeEpGhPpuOBBaFkgRM6IJd9T/4YPDePvnrb7oJsrNh3jxLAsZEAVf/y1V1JbDSb91kv+U/uxlDtArUA8i7ft68wKWFKVNw+tmvWgWPPAJnnVUhsRpjwitSGotNOSvqqj8l5VRbQKFeQDNnOuPvjBtXofEaY8LHEkEVEKhReMqUwOMAefv+p6RQuBfQ4cPOzVbXXAONG1foMRhjwscSQSUXrCsoFHHVH8zChfDzzzBhQoXEboyJDK6ONeQG6zVUUGKic/L3l5DgXOmHTBV69oScHNiwwckexpgqIyy9hkzFKKpRuERSU+Hzz53SgCUBY6KKJYJKrtiuoKGaOdOZyP3668sckzGmcrFEUImUplE4JAcPOu0Do0ZV/GQxxpiws0RQSZRro7C/+fOdmwqskdiYqGSNxZVEuTUK+1OFrl2hVi2nncAYUyWFZWIaU77KrVHY34cfwsaN8NxzZdyRMaaysqqhSqLcGoX9zZwJ9evDyJFl3JExprKyRFBJFNkovHs3DB/ujA20dm3oO92/H5YscYYjrVevPMM1xlQilggiUKDeQQHHB3o2j5Qjs6BTJ3j9dcjNhQsvhIkT4cSJ4r/oxRed7W65xe1DMsZEMlWtVI+ePXtqVTZ/vmpsrKrTius8YmOd9QVs3ao6YICzwaBBqt98o/rzz6q33OKs695dddOm4F+0a5dq+/aqvXu7eTjGmAgBpGqQ86qVCCJMUfMIAM48AVOnOj19vvjCaeRdvdqpFqpbF559Fv7v/5zJZXr2hKefdvJJdrZTbXT//c6kM/HxsG0b3HNPRR+iMSbCWPfRCFOtWhGzhx08BJdf7swaNny4c5Jv0SLwjvbscYaSXrnSSRrp6XDoENSoAf36waWXwpAh0LGjq8djjIkM1n20EomPD3y/QOe4g3DxL+Gzz5yZZYobCqJ5c/jvf+GZZ2D2bLj2Wufkf9FFdvewMaYAKxGE0YIFTpXPrl1OAvAOC+E/e1irOj+xofklNMn8El56CYYNC0/AxphKy0YfjUChDhnRPW4fXza9kCbfbYRlyywJGGPKnZUIwiSkISN+/NGpyklLcxqAL7mkAiM0xlQlYSsRiMhgEdkqImkiMinINteIyGYR2SQi/3EznkhS7JAR338PAwfC9u3w2muWBIwxrnGtsVhEYoAZwMVAJrBeRJar6mafbdoBDwB9VfWAiJzhVjyRJlijcHw8zo1hF1/sZIXXX4f+/Ss8PmNM9HCzRNALSFPV7ap6ElgE+Fdw3wzMUNUDAKr6o4vxRJQih4x4+23YtMlpLLAkYIxxmZuJoBWQ4bOc6Vnnqz3QXkQ+EJF1IjI40I5EZLyIpIpI6t69e10Kt2IFHDLCO4/A3LnQqBFcdVW4wzTGRIFw9xqqDrQDBgKjgOdE5DT/jVR1lqomq2py06ZNKzjEsgk0bpBXSorTMJyX5zynpABZWfDqq85sYbVqhSVmY0x0cfOGst1Aa5/lOM86X5nAx6qaDewQkW04iWG9i3FVGG8XUe89Ab5dRIPOILZkCRw/DmPGVESIxhjjaolgPdBORNqISE1gJLDcb5tlOKUBROR0nKqi7S7GVKGKHYOTDTIAABXDSURBVDcokDlznGEfkgP28jLGmHJXbCIQkStEpMQJQ1VzgNuBVcAWYImqbhKRh0RkqGezVcB+EdkMrAF+p6r7S/pdkarEs4p9840zY9iYMU7DgTHGVIBQqoauBZ4QkaXA86r6dag7V9WVwEq/dZN9Xitwj+dR5RTZRTSQuXOdxoTixhEyxphyVOyVvqpeDyQB3wJzROQjTy+e+q5HV8kV2UXUX16eM1HMJZdAy5YVEp8xxkCIbQSqegh4GedegBbAlcBnInKHi7FVekV2EfW3Zg1kZMDo0RUepzEmuoXSRjBURF4F1gI1gF6qeinQDbjX3fAqj2DdRAN2EQ1k7lxo2NAGlTPGVLhQ2giuAh5X1Xd9V6rqUREZ505YlUupuon6OnwYli512gbq1HEtTmOMCSSUqqE/A594F0SkjogkAqjqaleiqmRK1U3U18svOx+waiFjTBiEkgheAvJ8lnM964xHibuJ+pszB9q1g969yyskY4wJWSiJoLpn0DgAPK9ruhdS5ROsO2jQbqK+tm+Hd991SgN274AxJgxCSQR7fW4AQ0SGAfvcC6nyKVE3UX8vvugkgBtucCU2Y4wpTiiNxROABSLyNCA4I4r+2tWoKhlvg7D//MPFNhR77x248MIQiw/GGFP+ik0EqvotcL6I1PMs/+x6VJVQSkqIPYR8ffkl7NgBkycXv60xxrgkpNFHRWQIcA5QWzz12Kr6kItxRYdvv3Weu3ULbxzGmKgWyg1lz+KMN3QHTtXQCCDB5biig3cgosTEsIZhjIluoTQW91HVXwMHVPUvQG+c4aJNWaWnQ4MGcFqhuXiMMabChJIIjnuej4pISyAbZ7yhqFTUjGMllp5+aiAiY4wJk1DaCFZ4po+cBnwGKPCcq1FFqDIPJeEvPd2qhYwxYVdkicAzIc1qVT2oqktx2gY6+M4pEE3KPJSEL1VLBMaYiFBkIlDVPGCGz/IJVc1yPaoIVeahJHwdOOAMNmeJwBgTZqG0EawWkatErCK7TENJ+EtPd54tERhjwiyURHALziBzJ0TkkIgcFpFDLscVkco0lIQ/6zpqjIkQoUxVWV9Vq6lqTVVt4FluEMrORWSwiGwVkTQRmRTg/TEisldEvvA8birNQVSUEs04VhwrERhjIkSxvYZEpH+g9f4T1QT4XAxO+8LFQCawXkSWq+pmv00Xq+rtIcYbdkUOJfH667BhA0wqlPMKS0+HevWgUaPyDM8YY0oslO6jv/N5XRvoBXwKXFjM53oBaaq6HUBEFgHDAP9EUHXMng2vvQa/+x3ExBS9rbfHkDW9GGPCLJSqoSt8HhcDnYEDIey7Fc5IpV6ZnnX+rhKRL0XkZRFpHVLUkSozE06cOFXtUxTrOmqMiRChNBb7ywQ6ltP3rwASVbUr8BYwN9BGIjJeRFJFJHXv3r3l9NUuyPDkvc0hFHosERhjIkQobQRP4dxNDE7i6I5zh3FxdgO+V/hxnnX5VHW/z+Js4JFAO1LVWcAsgOTkZA20TdidPAl79jivt2yBK64Ivu3Bg3DokCUCY0xECKWNINXndQ6wUFU/COFz64F2ItIGJwGMBK7z3UBEWqjq957FocCWEPYbmb77zrlbGIovEViPIWNMBAklEbwMHFfVXHB6A4lIrKoeLepDqpojIrcDq4AY4HlV3SQiDwGpqrocuNMzDWYO8BMwpgzHEl7eaqGaNZ0SQVEsERhjIkhIdxYDdXyW6wBvh7JzVV2pqu1Vta2qTvGsm+xJAqjqA6p6jqp2U9VBqvp1SQ/ALSUeZdSbCPr1cxKBFlGDZYnAGBNBQkkEtX2np/S8ji1i+0rPO8rozp3O+dw7ymiRycCbCC65xBlDaPfu4Nump0PdutC4cXmGbYwxpRJKIjgiIj28CyLSEzjmXkjhV6pRRjMynAlmzj3XWS6qesjuITDGRJBQ2gjuBl4Ske9wpqpsjjN1ZZVVqlFGMzKgdWvo1MlZ3rwZLr448LY7d1q1kDEmYhSbCFR1vYh0AM72rNqqqtnuhhVe8fGnxoTzXx+UNxGccYYzbERxJYK+fcsapjHGlItQJq//DVBXVTeq6kagnojc5n5o4VOqUUa9iUDEKRUESwQHDzoPKxEYYyJEKG0EN6vqQe+Cqh4AbnYvpPAr8Sijx47Bvn1OIgDo2DH4vQQ2/LQxJsKEkghifCel8YwqWtO9kCJDSopTg5OX5zwXOdR0Zqbz7E0EnTo5iSHQcBjWddQYE2FCSQRvAItF5CIRuQhYCLzubliVjLfrqG+JAAJXD3kTQUKC62EZY0woQkkE9wP/AyZ4Hl9R8AYzU9JEEBsLp59eIaEZY0xxQhmGOg/4GEjHmWPgQirzmEBu8CaCuDjnuXVr54axQInA23XU7iEwxkSIoN1HRaQ9MMrz2AcsBlDVQRUTWiWSkQFNm0Lt2s5ytWrQoUPgBmMbftoYE2GKKhF8jXP1f7mqXqCqTwG5FRNWJePtOuorWBdSSwTGmAhTVCIYDnwPrBGR5zwNxVafEUigRNCxo9Ob6NChU+uysuDAAUsExpiIEjQRqOoyVR0JdADW4Aw1cYaI/EtELqmoACuFYIkA4GufAVXtHgJjTAQKpbH4iKr+R1WvwJll7HOcnkQGnJFGs7ICVw1BwXYC6zpqjIlAJZqzWFUPqOosVb3IrYAqHf+uo15nnll4khorERhjIlBpJq83voIlgurVoX37gokgPR3q1HF6GBljTISwRFBWwRIBFB5zyOYhMMZEIEsEZZWR4ZzYW7Ys/F7HjrBjhzMoHVjXUWNMRIr6RFDiuYn9ZWRAixZQo0bh9zp1ckat27bNWbZEYIyJQK4mAhEZLCJbRSRNRCYVsd1VIqIikuxmPP5KNTexv0BdR718xxw6fBh++skSgTEm4riWCDzDVc8ALgU6AaNEpFOA7eoDd+GMZ1ShSjU3sb+iEkH79k5RY8sW6zFkjIlYbpYIegFpqrpdVU8Ci4BhAbb7K/AwcNzFWAIq1dzEvlSLTgS1azvdSDdvtnsIjDERy81E0ArI8FnO9KzLJyI9gNaq+lpROxKR8SKSKiKpewNN9lJKweYgLnJuYl8HDjhFiGCJAJzqoS1bbEIaY0zECltjsYhUAx4D7i1uW89NbMmqmty0HPvgl2puYl9FdR316tTJaSxOS3NKCGecUapYjTHGLW4mgt2A7xkyzrPOqz7QGVgrIunA+cDyimwwLvHcxP5CSQQdO0J2NqxebfcQGGMiUtD5CMrBeqCdiLTBSQAjgeu8b6pqFpA/TZeIrAXuU9VUF2MqJCWlBCd+f6GWCAA2boTBg0v5RcYY4x7XSgSqmgPcDqzCmdFsiapuEpGHRGSoW99boTIynKEkmjULvk2HDqdeW/uAMSYCuVkiQFVXAiv91k0Osu1AN2NxRUYGtGoFMTHBt6lf3ykxZGRYIjDGRKSov7O4TDIzi64W8vLeWGZdR40xEcgSQVkUdQ+BL28isBKBMSYCWSIoLdXQSwR9+jj9Utu3dz8uY4wpIUsEpbV3L5w4EVoiGDECdu+Gxo3dj8sYY0rIEkFphdJ11EsETjvN3XiMMaaULBGUVkkSgTHGRDBLBKXlTQRxceGNwxhjysgSQWllZECtWjb/sDGm0rNEUFoZGU5pwMYOMsZUcpYISivUewiMMSbCWSIoLUsExpgqwhJBaeTmOvcFWCIwxlQBlghKY88eJxlYIjDGVAGWCErD7iEwxlQhlghKwxKBMaYKsURQGpYIjDFViCWC0sjIgLp1bfwgY0yV4OoMZZXe//4Hhw45dw83bQpnnAENG57qOmo3kxljqgBLBMF8+SVcdFHh9TVqOHMRDBpU8TEZY4wLLBEEM3Uq1KsHb74Jhw/Djz86cxB4n0eMCHeExhhTLlxNBCIyGHgSiAFmq+pUv/cnAL8BcoGfgfGqutnNmEKSlgaLF8N990Hv3uGOxhhjXOVaY7GIxAAzgEuBTsAoEenkt9l/VLWLqnYHHgEecyueEnnkEacK6Le/DXckxhjjOjd7DfUC0lR1u6qeBBYBw3w3UNVDPot1AXUxntDs3g1z58KNN0Lz5uGOxhhjXOdm1VArIMNnORM4z38jEfkNcA9QE7gw0I5EZDwwHiA+Pr7cAy3gscec4SN+9zt3v8cYYyJE2O8jUNUZqtoWuB/4Q5BtZqlqsqomN3VzIpj9+2HmTBg1Ctq0ce97jDEmgriZCHYDvrfexnnWBbMI+JWL8RTvqafgyBGYNCmsYRhjTEVyMxGsB9qJSBsRqQmMBJb7biAi7XwWhwDfuBhP0Q4fhunTYdgwOOecsIVhjDEVzbU2AlXNEZHbgVU43UefV9VNIvIQkKqqy4HbReQXQDZwABjtVjzFmjULDhyABx4IWwjGGBMOohr+jjolkZycrKmpqeW70xMnnDaBjh1h9ery3bcxxkQAEflUVZMDvWd3FoPTXfT772HevHBHYowxFS7svYbCLicHHn4Yzj0XLgzYe9UYY6o0SwRr1sD27TBxoo0maoyJSpYIPvrISQCXXBLuSIwxJiwsEaxb53QXbdAg3JEYY0xYRHciUHUSwfnnhzsSY4wJm+hOBN9849w7YInAGBPFojsRfPyx82yJwBgTxaI7EaxbB/XrQ4cO4Y7EGGPCxhJBr14QExPuSIwxJmyiIhEsWACJiVCtmvO8YAFw9Chs2GDVQsaYqFflh5hYsADGj3fO+wA7dzrLTb/+lEtycy0RGGOiXpUvETz44Kkk4HX0KKQ+vc5ZOK/QpGnGGBNVqnwi2LUr8PqzD66Dtm3BzRnPjDGmEqjyiSDYFMd9Y+xGMmOMgShIBFOmQGxswXVn1c6kee53Vi1kjDFEQSJISXEmH0tIcMaWS0iAf4/3tA9YicAYY6p+IgAnGaSnQ16e89y/xjqoVQu6dQt3aMYYE3ZRkQgKWbcOevaEmjXDHYkxxoRd9CWCkyfh00+tWsgYYzxcTQQiMlhEtopImohMCvD+PSKyWUS+FJHVIpLgZjwAfPklHD9uicAYYzxcSwQiEgPMAC4FOgGjRKST32afA8mq2hV4GXjErXjyrbOGYmOM8eVmiaAXkKaq21X1JLAIGOa7gaquUVXvfb/rgDgX43F8/DG0aAFx7n+VMcZUBm4mglZAhs9ypmddMOOA1wO9ISLjRSRVRFL37t1btqi8M5LZRPXGGANESGOxiFwPJAPTAr2vqrNUNVlVk5uWZUiIffsgLc2qhYwxxoebo4/uBlr7LMd51hUgIr8AHgQGqOoJF+OxGcmMMSYAN0sE64F2ItJGRGoCI4HlvhuISBIwExiqqj+6GItj3TpnEpqePV3/KmOMqSxcSwSqmgPcDqwCtgBLVHWTiDwkIkM9m00D6gEvicgXIrI8yO7Kx7p10LUr1K3r6tcYY0xl4urENKq6Eljpt26yz+tfuPn9BeTlwSefOONNGGOMyRcRjcUV4uuv4dAhG3HUGGP8RE8isBvJjDEmoOhJBKefDsOGQbt24Y7EGGMiSpWfvD7f0KHOwxhjTAHRUyIwxhgTkCUCY4yJcpYIjDEmylkiMMaYKGeJwBhjopwlAmOMiXKWCIwxJspZIjDGmCgnqhruGEpERPYCO0v58dOBfeUYTmURrccN0XvsdtzRJZTjTlDVgDN7VbpEUBYikqqqyeGOo6JF63FD9B67HXd0KetxW9WQMcZEOUsExhgT5aItEcwKdwBhEq3HDdF77Hbc0aVMxx1VbQTGGGMKi7YSgTHGGD+WCIwxJspFTSIQkcEislVE0kRkUrjjcYuIPC8iP4rIRp91jUXkLRH5xvPcKJwxukFEWovIGhHZLCKbROQuz/oqfewiUltEPhGRDZ7j/otnfRsR+djz975YRGqGO1Y3iEiMiHwuIv/1LFf54xaRdBH5SkS+EJFUz7oy/Z1HRSIQkRhgBnAp0AkYJSKdwhuVa+YAg/3WTQJWq2o7YLVnuarJAe5V1U7A+cBvPL/jqn7sJ4ALVbUb0B0YLCLnAw8Dj6vqWcABYFwYY3TTXcAWn+VoOe5Bqtrd596BMv2dR0UiAHoBaaq6XVVPAouAYWGOyRWq+i7wk9/qYcBcz+u5wK8qNKgKoKrfq+pnnteHcU4Orajix66Onz2LNTwPBS4EXvasr3LHDSAiccAQYLZnWYiC4w6iTH/n0ZIIWgEZPsuZnnXRopmqfu95vQdoFs5g3CYiiUAS8DFRcOye6pEvgB+Bt4BvgYOqmuPZpKr+vT8BTATyPMtNiI7jVuBNEflURMZ71pXp7zx6Jq83gHMFKSJVts+wiNQDlgJ3q+oh5yLRUVWPXVVzge4ichrwKtAhzCG5TkQuB35U1U9FZGC446lgF6jqbhE5A3hLRL72fbM0f+fRUiLYDbT2WY7zrIsWP4hICwDP849hjscVIlIDJwksUNVXPKuj4tgBVPUgsAboDZwmIt4Lvar4994XGCoi6ThVvRcCT1L1jxtV3e15/hEn8feijH/n0ZII1gPtPD0KagIjgeVhjqkiLQdGe16PBv4vjLG4wlM//G9gi6o+5vNWlT52EWnqKQkgInWAi3HaR9YAV3s2q3LHraoPqGqcqibi/D//T1VTqOLHLSJ1RaS+9zVwCbCRMv6dR82dxSJyGU6dYgzwvKpOCXNIrhCRhcBAnGFpfwD+BCwDlgDxOEN4X6Oq/g3KlZqIXAC8B3zFqTrj3+O0E1TZYxeRrjiNgzE4F3ZLVPUhETkT50q5MfA5cL2qnghfpO7xVA3dp6qXV/Xj9hzfq57F6sB/VHWKiDShDH/nUZMIjDHGBBYtVUPGGGOCsERgjDFRzhKBMcZEOUsExhgT5SwRGGNMlLNEYIyHiOR6RnT0PsptgDoRSfQdEdaYSGJDTBhzyjFV7R7uIIypaFYiMKYYnvHfH/GMAf+JiJzlWZ8oIv8TkS9FZLWIxHvWNxORVz1zBGwQkT6eXcWIyHOeeQPe9NwJjIjc6ZlH4UsRWRSmwzRRzBKBMafU8asautbnvSxV7QI8jXOHOsBTwFxV7QosAKZ71k8H3vHMEdAD2ORZ3w6YoarnAAeBqzzrJwFJnv1McOvgjAnG7iw2xkNEflbVegHWp+NM/rLdM7DdHlVtIiL7gBaqmu1Z/72qni4ie4E436ENPENjv+WZOAQRuR+ooap/E5E3gJ9xhgJZ5jO/gDEVwkoExoRGg7wuCd8xb3I51UY3BGcGvR7Aep/RM42pEJYIjAnNtT7PH3lef4gz8iVACs6gd+BMFXgr5E8a0zDYTkWkGtBaVdcA9wMNgUKlEmPcZFcexpxSxzPTl9cbqurtQtpIRL7Euaof5Vl3B/CCiPwO2AuM9ay/C5glIuNwrvxvBb4nsBhgvidZCDDdM6+AMRXG2giMKYanjSBZVfeFOxZj3GBVQ8YYE+WsRGCMMVHOSgTGGBPlLBEYY0yUs0RgjDFRzhKBMcZEOUsExhgT5f4f/cHztH0mZAkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvS6MbyTfFAg"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV90a0jRfFAh"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qerH8oCfFAh",
    "outputId": "3b563e99-7cde-4699-bc42-0b428027675e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 49,691,466\n",
      "Trainable params: 49,661,514\n",
      "Non-trainable params: 29,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#<Compile your model again (using the same hyper-parameters)>\n",
    "# Reinitialize VGG-16\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activations.relu))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "voNE8YbEfFAi",
    "outputId": "3e3fb748-5568-439a-edf7-1d07150d9bb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 43s 26ms/step - loss: 2.3960 - acc: 0.1914\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 1.6572 - acc: 0.4027\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 1.4500 - acc: 0.4823\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 1.3160 - acc: 0.5391\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 1.2254 - acc: 0.5722\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 1.1465 - acc: 0.6028\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 1.0887 - acc: 0.6242\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 1.0468 - acc: 0.6456\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.9937 - acc: 0.6613\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.9580 - acc: 0.6801\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.8944 - acc: 0.6967\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.8726 - acc: 0.7091\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.8341 - acc: 0.7184\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.8097 - acc: 0.7296\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.7755 - acc: 0.7370\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 39s 25ms/step - loss: 0.7528 - acc: 0.7444\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 39s 25ms/step - loss: 0.7502 - acc: 0.7546\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.7032 - acc: 0.7650\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.6866 - acc: 0.7717\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.6784 - acc: 0.7725\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.6734 - acc: 0.7798\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.6326 - acc: 0.7879\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.6319 - acc: 0.7892\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.6110 - acc: 0.7974\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.6151 - acc: 0.8016\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5845 - acc: 0.8070\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5847 - acc: 0.8066\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5558 - acc: 0.8124\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5628 - acc: 0.8145\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5488 - acc: 0.8209\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5360 - acc: 0.8243\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5396 - acc: 0.8259\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5300 - acc: 0.8300\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5293 - acc: 0.8298\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5000 - acc: 0.8326\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4864 - acc: 0.8368\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5004 - acc: 0.8377\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4735 - acc: 0.8421\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4797 - acc: 0.8446\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4596 - acc: 0.8481\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4502 - acc: 0.8497\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4500 - acc: 0.8526\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4475 - acc: 0.8546\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4423 - acc: 0.8549\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4184 - acc: 0.8605\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4169 - acc: 0.8611\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4253 - acc: 0.8598\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4194 - acc: 0.8642\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4144 - acc: 0.8650\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.3916 - acc: 0.8694\n"
     ]
    }
   ],
   "source": [
    "#<Train your model on the entire training set (50K samples)>\n",
    "#<Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n",
    "#<Do NOT use the validation_data option (because now you do not have validation data)>\n",
    "#Train Model\n",
    "history = model.fit(x_train, y_train_vec, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br14Vr47fFAi"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e36WdXq5fFAj",
    "outputId": "79f7475a-ce88-4b5e-e54c-67c57c92135a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.4778 - acc: 0.8471\n",
      "loss = 0.47781363129615784\n",
      "accuracy = 0.847100019454956\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HM4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
