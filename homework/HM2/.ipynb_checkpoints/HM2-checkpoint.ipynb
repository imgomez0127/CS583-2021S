{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM2: Numerical Optimization for Logistic Regression.\n",
    "\n",
    "### Name: Ian Gomez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read the lecture note: [click here](https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Logistic/paper/logistic.pdf)\n",
    "\n",
    "2. Read, complete, and run my code.\n",
    "\n",
    "3. **Implement mini-batch SGD** and evaluate the performance.\n",
    "\n",
    "4. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain **the code** and **the output after execution**.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "5. Upload this .HTML file to your Google Drive, Dropbox, or your Github repo.  (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "6. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM2/HM2.html\n",
    "\n",
    "\n",
    "## Grading criteria:\n",
    "\n",
    "1. When computing the ```gradient``` and ```objective function value``` using a batch of samples, use **matrix-vector multiplication** rather than a FOR LOOP of **vector-vector multiplications**.\n",
    "\n",
    "2. Plot ```objective function value``` against ```epochs```. In the plot, compare GD, SGD, and MB-SGD (with $b=8$ and $b=64$). The plot must look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Diabete dataset from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes\n",
    "- Load the data using sklearn.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (768, 8)\n",
      "Shape of y: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy\n",
    "\n",
    "x_sparse, y = datasets.load_svmlight_file('diabetes')\n",
    "x = x_sparse.todense()\n",
    "\n",
    "print('Shape of x: ' + str(x.shape))\n",
    "print('Shape of y: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Partition to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (640, 8)\n",
      "Shape of x_test: (128, 8)\n",
      "Shape of y_train: (640, 1)\n",
      "Shape of y_test: (128, 1)\n"
     ]
    }
   ],
   "source": [
    "# partition the data to training and test sets\n",
    "n = x.shape[0]\n",
    "n_train = 640\n",
    "n_test = n - n_train\n",
    "\n",
    "rand_indices = numpy.random.permutation(n)\n",
    "train_indices = rand_indices[0:n_train]\n",
    "test_indices = rand_indices[n_train:n]\n",
    "\n",
    "x_train = x[train_indices, :]\n",
    "x_test = x[test_indices, :]\n",
    "y_train = y[train_indices].reshape(n_train, 1)\n",
    "y_test = y[test_indices].reshape(n_test, 1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "[[-0.04838893 -0.07151944 -0.02868179 -0.05893791 -0.08448332  0.08014436\n",
      "  -0.0530847  -0.01253443]]\n",
      "test std = \n",
      "[[1.07172837 0.82269766 1.06737203 1.0315592  0.84257045 1.18650167\n",
      "  0.98183711 0.95352618]]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Add a dimension of all ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (640, 9)\n",
      "Shape of x_test: (128, 9)\n"
     ]
    }
   ],
   "source": [
    "n_train, d = x_train.shape\n",
    "x_train = numpy.concatenate((x_train, numpy.ones((n_train, 1))), axis=1)\n",
    "\n",
    "n_test, d = x_test.shape\n",
    "x_test = numpy.concatenate((x_test, numpy.ones((n_test, 1))), axis=1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     objective function value (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(-yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.log(1 + vec1) # n-by-1 matrix\n",
    "    loss = numpy.mean(vec2) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    return loss + reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial objective function value = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# initialize w\n",
    "d = x_train.shape[1]\n",
    "w = numpy.zeros((d, 1))\n",
    "\n",
    "# evaluate the objective function value at w\n",
    "lam = 1E-6\n",
    "objval0 = objective(w, x_train, y_train, lam)\n",
    "print('Initial objective function value = ' + str(objval0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ is $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     g: g: d-by-1 matrix, full gradient\n",
    "def gradient(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.divide(yx, 1+vec1) # n-by-d matrix\n",
    "    vec3 = -numpy.mean(vec2, axis=0).reshape(d, 1) # d-by-1 matrix\n",
    "    g = vec3 + lam * w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_iter: integer, the maximal iterations\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each iteration's objective value\n",
    "def grad_descent(x, y, lam, stepsize, max_iter=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_iter) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        objval = objective(w, x, y, lam)\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at t=' + str(t) + ' is ' + str(objval))\n",
    "        g = gradient(w, x, y, lam)\n",
    "        w -= stepsize * g\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at t=0 is 0.6931471805599453\n",
      "Objective value at t=1 is 0.5827668703562244\n",
      "Objective value at t=2 is 0.5380408688748962\n",
      "Objective value at t=3 is 0.5138567378443023\n",
      "Objective value at t=4 is 0.49868553123908405\n",
      "Objective value at t=5 is 0.4883634395004791\n",
      "Objective value at t=6 is 0.4809803255249772\n",
      "Objective value at t=7 is 0.475517336456729\n",
      "Objective value at t=8 is 0.47137402414768537\n",
      "Objective value at t=9 is 0.46817131264737033\n",
      "Objective value at t=10 is 0.4656577425903729\n",
      "Objective value at t=11 is 0.46366019983086826\n",
      "Objective value at t=12 is 0.4620559982861979\n",
      "Objective value at t=13 is 0.46075612389869713\n",
      "Objective value at t=14 is 0.4596947165139871\n",
      "Objective value at t=15 is 0.458822227882775\n",
      "Objective value at t=16 is 0.45810083969978477\n",
      "Objective value at t=17 is 0.4575013195910352\n",
      "Objective value at t=18 is 0.4570008183532302\n",
      "Objective value at t=19 is 0.45658129830502914\n",
      "Objective value at t=20 is 0.4562283936815651\n",
      "Objective value at t=21 is 0.455930572252173\n",
      "Objective value at t=22 is 0.45567851041187357\n",
      "Objective value at t=23 is 0.4554646218100256\n",
      "Objective value at t=24 is 0.45528269790448683\n",
      "Objective value at t=25 is 0.4551276311210318\n",
      "Objective value at t=26 is 0.4549951996761945\n",
      "Objective value at t=27 is 0.4548818989173878\n",
      "Objective value at t=28 is 0.4547848080978841\n",
      "Objective value at t=29 is 0.4547014843895339\n",
      "Objective value at t=30 is 0.45462987800876187\n",
      "Objective value at t=31 is 0.4545682638366341\n",
      "Objective value at t=32 is 0.4545151860182356\n",
      "Objective value at t=33 is 0.45446941284478504\n",
      "Objective value at t=34 is 0.45442989983356186\n",
      "Objective value at t=35 is 0.4543957593818839\n",
      "Objective value at t=36 is 0.4543662357218883\n",
      "Objective value at t=37 is 0.4543406841713037\n",
      "Objective value at t=38 is 0.4543185538824809\n",
      "Objective value at t=39 is 0.45429937345274496\n",
      "Objective value at t=40 is 0.4542827388848358\n",
      "Objective value at t=41 is 0.45426830348503067\n",
      "Objective value at t=42 is 0.4542557693647092\n",
      "Objective value at t=43 is 0.4542448802732756\n",
      "Objective value at t=44 is 0.4542354155400271\n",
      "Objective value at t=45 is 0.4542271849424496\n",
      "Objective value at t=46 is 0.45422002435061626\n",
      "Objective value at t=47 is 0.45421379202343853\n",
      "Objective value at t=48 is 0.4542083654537504\n",
      "Objective value at t=49 is 0.45420363867653785\n",
      "Objective value at t=50 is 0.45419951996883756\n",
      "Objective value at t=51 is 0.45419592988152785\n",
      "Objective value at t=52 is 0.4541927995528787\n",
      "Objective value at t=53 is 0.4541900692617323\n",
      "Objective value at t=54 is 0.45418768718480873\n",
      "Objective value at t=55 is 0.45418560832817534\n",
      "Objective value at t=56 is 0.4541837936075205\n",
      "Objective value at t=57 is 0.4541822090557495\n",
      "Objective value at t=58 is 0.45418082513965374\n",
      "Objective value at t=59 is 0.45417961617013786\n",
      "Objective value at t=60 is 0.45417855979278593\n",
      "Objective value at t=61 is 0.4541776365474899\n",
      "Objective value at t=62 is 0.4541768294875059\n",
      "Objective value at t=63 is 0.454176123849697\n",
      "Objective value at t=64 is 0.4541755067689034\n",
      "Objective value at t=65 is 0.45417496703038535\n",
      "Objective value at t=66 is 0.4541744948551443\n",
      "Objective value at t=67 is 0.45417408171365387\n",
      "Objective value at t=68 is 0.4541737201641587\n",
      "Objective value at t=69 is 0.4541734037122349\n",
      "Objective value at t=70 is 0.45417312668875814\n",
      "Objective value at t=71 is 0.4541728841438233\n",
      "Objective value at t=72 is 0.4541726717544898\n",
      "Objective value at t=73 is 0.4541724857445215\n",
      "Objective value at t=74 is 0.4541723228145341\n",
      "Objective value at t=75 is 0.4541721800811791\n",
      "Objective value at t=76 is 0.45417205502417785\n",
      "Objective value at t=77 is 0.4541719454401763\n",
      "Objective value at t=78 is 0.45417184940253186\n",
      "Objective value at t=79 is 0.45417176522625596\n",
      "Objective value at t=80 is 0.45417169143744607\n",
      "Objective value at t=81 is 0.4541716267466227\n",
      "Objective value at t=82 is 0.4541715700254679\n",
      "Objective value at t=83 is 0.45417152028652397\n",
      "Objective value at t=84 is 0.4541714766654757\n",
      "Objective value at t=85 is 0.4541714384056782\n",
      "Objective value at t=86 is 0.4541714048446476\n",
      "Objective value at t=87 is 0.45417137540225966\n",
      "Objective value at t=88 is 0.4541713495704405\n",
      "Objective value at t=89 is 0.4541713269041575\n",
      "Objective value at t=90 is 0.45417130701354586\n",
      "Objective value at t=91 is 0.4541712895570252\n",
      "Objective value at t=92 is 0.45417127423528236\n",
      "Objective value at t=93 is 0.4541712607860085\n",
      "Objective value at t=94 is 0.4541712489792965\n",
      "Objective value at t=95 is 0.4541712386136152\n",
      "Objective value at t=96 is 0.4541712295122863\n",
      "Objective value at t=97 is 0.4541712215204023\n",
      "Objective value at t=98 is 0.45417121450212855\n",
      "Objective value at t=99 is 0.45417120833834207\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 1.0\n",
    "w, objvals_gd = grad_descent(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     xi: 1-by-d matrix\n",
    "#     yi: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    d = xi.shape[0]\n",
    "    yx = yi * xi # 1-by-d matrix\n",
    "    yxw = float(numpy.dot(yx, w)) # scalar\n",
    "    \n",
    "    # calculate objective function Q_i\n",
    "    loss = numpy.log(1 + numpy.exp(-yxw)) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    obj = loss + reg\n",
    "    \n",
    "    # calculate stochastic gradient\n",
    "    g_loss = -yx.T / (1 + numpy.exp(yxw)) # d-by-1 matrix\n",
    "    g = g_loss + lam * w # d-by-1 matrix\n",
    "    \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     objvals: record of each iteration's objective value\n",
    "def sgd(x, y, lam, stepsize, max_epoch=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        # randomly shuffle the samples\n",
    "        rand_indices = numpy.random.permutation(n)\n",
    "        x_rand = x[rand_indices, :]\n",
    "        y_rand = y[rand_indices, :]\n",
    "        \n",
    "        objval = 0 # accumulate the objective values\n",
    "        for i in range(n):\n",
    "            xi = x_rand[i, :] # 1-by-d matrix\n",
    "            yi = float(y_rand[i, :]) # scalar\n",
    "            obj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            objval += obj\n",
    "            w -= stepsize * g\n",
    "        \n",
    "        stepsize *= 0.9 # decrease step size\n",
    "        objval /= n\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval))\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.4948217709869248\n",
      "Objective value at epoch t=1 is 0.4970838065289874\n",
      "Objective value at epoch t=2 is 0.48736452145032194\n",
      "Objective value at epoch t=3 is 0.4884415176947575\n",
      "Objective value at epoch t=4 is 0.4866544723231708\n",
      "Objective value at epoch t=5 is 0.4847975792820545\n",
      "Objective value at epoch t=6 is 0.48509986949750517\n",
      "Objective value at epoch t=7 is 0.4746190801102833\n",
      "Objective value at epoch t=8 is 0.47670019389592133\n",
      "Objective value at epoch t=9 is 0.4729353707436199\n",
      "Objective value at epoch t=10 is 0.4749674033003166\n",
      "Objective value at epoch t=11 is 0.47143151113241155\n",
      "Objective value at epoch t=12 is 0.46985008619283414\n",
      "Objective value at epoch t=13 is 0.46969806196339015\n",
      "Objective value at epoch t=14 is 0.46620874466234374\n",
      "Objective value at epoch t=15 is 0.46725497287812245\n",
      "Objective value at epoch t=16 is 0.4633389759197189\n",
      "Objective value at epoch t=17 is 0.4653810853193693\n",
      "Objective value at epoch t=18 is 0.46339708332972196\n",
      "Objective value at epoch t=19 is 0.462700670451916\n",
      "Objective value at epoch t=20 is 0.460249348646495\n",
      "Objective value at epoch t=21 is 0.4616804515160637\n",
      "Objective value at epoch t=22 is 0.45982709849552467\n",
      "Objective value at epoch t=23 is 0.4599569337803615\n",
      "Objective value at epoch t=24 is 0.4591846903321574\n",
      "Objective value at epoch t=25 is 0.4586303873422882\n",
      "Objective value at epoch t=26 is 0.4584844525883501\n",
      "Objective value at epoch t=27 is 0.4580629338285367\n",
      "Objective value at epoch t=28 is 0.4575122376060035\n",
      "Objective value at epoch t=29 is 0.4573153449062068\n",
      "Objective value at epoch t=30 is 0.4571712074943143\n",
      "Objective value at epoch t=31 is 0.4568895933557687\n",
      "Objective value at epoch t=32 is 0.45654701605286174\n",
      "Objective value at epoch t=33 is 0.4562764095128965\n",
      "Objective value at epoch t=34 is 0.45603316126670856\n",
      "Objective value at epoch t=35 is 0.4559276842778058\n",
      "Objective value at epoch t=36 is 0.4556689457046838\n",
      "Objective value at epoch t=37 is 0.45559666435708845\n",
      "Objective value at epoch t=38 is 0.4554294783748257\n",
      "Objective value at epoch t=39 is 0.4553254641192307\n",
      "Objective value at epoch t=40 is 0.4552069459454445\n",
      "Objective value at epoch t=41 is 0.45509415128648395\n",
      "Objective value at epoch t=42 is 0.45500881746219013\n",
      "Objective value at epoch t=43 is 0.4549151751624857\n",
      "Objective value at epoch t=44 is 0.4548557755014202\n",
      "Objective value at epoch t=45 is 0.4547870717352566\n",
      "Objective value at epoch t=46 is 0.45472736084233506\n",
      "Objective value at epoch t=47 is 0.454671369010115\n",
      "Objective value at epoch t=48 is 0.4546207084851813\n",
      "Objective value at epoch t=49 is 0.4545767070730598\n",
      "Objective value at epoch t=50 is 0.454537691557542\n",
      "Objective value at epoch t=51 is 0.45450173392613874\n",
      "Objective value at epoch t=52 is 0.45446860675225303\n",
      "Objective value at epoch t=53 is 0.4544393740123618\n",
      "Objective value at epoch t=54 is 0.4544108752195015\n",
      "Objective value at epoch t=55 is 0.45438765376501467\n",
      "Objective value at epoch t=56 is 0.45436698978493445\n",
      "Objective value at epoch t=57 is 0.4543474260181187\n",
      "Objective value at epoch t=58 is 0.4543287186057535\n",
      "Objective value at epoch t=59 is 0.4543138803676444\n",
      "Objective value at epoch t=60 is 0.45429939062508157\n",
      "Objective value at epoch t=61 is 0.4542863441918904\n",
      "Objective value at epoch t=62 is 0.4542754091549403\n",
      "Objective value at epoch t=63 is 0.45426483658848893\n",
      "Objective value at epoch t=64 is 0.4542555379562515\n",
      "Objective value at epoch t=65 is 0.4542472648384899\n",
      "Objective value at epoch t=66 is 0.454239632884189\n",
      "Objective value at epoch t=67 is 0.4542327216385404\n",
      "Objective value at epoch t=68 is 0.45422672147743837\n",
      "Objective value at epoch t=69 is 0.4542211597061233\n",
      "Objective value at epoch t=70 is 0.4542161582864009\n",
      "Objective value at epoch t=71 is 0.4542116998636089\n",
      "Objective value at epoch t=72 is 0.4542076552858091\n",
      "Objective value at epoch t=73 is 0.4542040331928954\n",
      "Objective value at epoch t=74 is 0.45420071006050405\n",
      "Objective value at epoch t=75 is 0.45419782544448173\n",
      "Objective value at epoch t=76 is 0.4541951788897112\n",
      "Objective value at epoch t=77 is 0.45419279488707004\n",
      "Objective value at epoch t=78 is 0.4541906388002827\n",
      "Objective value at epoch t=79 is 0.4541887129094853\n",
      "Objective value at epoch t=80 is 0.4541869731641965\n",
      "Objective value at epoch t=81 is 0.4541854125207137\n",
      "Objective value at epoch t=82 is 0.454184005924193\n",
      "Objective value at epoch t=83 is 0.4541827385568557\n",
      "Objective value at epoch t=84 is 0.4541815991237953\n",
      "Objective value at epoch t=85 is 0.4541805707646672\n",
      "Objective value at epoch t=86 is 0.454179648019005\n",
      "Objective value at epoch t=87 is 0.45417881594789744\n",
      "Objective value at epoch t=88 is 0.45417806789696086\n",
      "Objective value at epoch t=89 is 0.4541773942384347\n",
      "Objective value at epoch t=90 is 0.45417678807027384\n",
      "Objective value at epoch t=91 is 0.45417624289164227\n",
      "Objective value at epoch t=92 is 0.45417575120974785\n",
      "Objective value at epoch t=93 is 0.45417530966244923\n",
      "Objective value at epoch t=94 is 0.45417491179132946\n",
      "Objective value at epoch t=95 is 0.45417455394505\n",
      "Objective value at epoch t=96 is 0.4541742316679621\n",
      "Objective value at epoch t=97 is 0.45417394179931964\n",
      "Objective value at epoch t=98 is 0.45417368060270913\n",
      "Objective value at epoch t=99 is 0.45417344588421465\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 0.1\n",
    "w, objvals_sgd = sgd(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD with SGD\n",
    "\n",
    "Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "epochs_gd = range(len(objvals_gd))\n",
    "epochs_sgd = range(len(objvals_sgd))\n",
    "\n",
    "line0, = plt.plot(epochs_gd, objvals_gd, '--b', LineWidth=4)\n",
    "line1, = plt.plot(epochs_sgd, objvals_sgd, '-r', LineWidth=2)\n",
    "plt.xlabel('Epochs', FontSize=20)\n",
    "plt.ylabel('Objective Value', FontSize=20)\n",
    "plt.xticks(FontSize=16)\n",
    "plt.yticks(FontSize=16)\n",
    "plt.legend([line0, line1], ['GD', 'SGD'], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('compare_gd_sgd.pdf', format='pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     X: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    xw = numpy.dot(X, w)\n",
    "    f = numpy.sign(xw)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification error is 0.2140625\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error\n",
    "f_train = predict(w, x_train)\n",
    "diff = numpy.abs(f_train - y_train) / 2\n",
    "error_train = numpy.mean(diff)\n",
    "print('Training classification error is ' + str(error_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification error is 0.234375\n"
     ]
    }
   ],
   "source": [
    "# evaluate test error\n",
    "f_test = predict(w, x_test)\n",
    "diff = numpy.abs(f_test - y_test) / 2\n",
    "error_test = numpy.mean(diff)\n",
    "print('Test classification error is ' + str(error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Mini-batch SGD (fill the code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Compute the objective $Q_I$ and its gradient using a batch of samples\n",
    "\n",
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     xi: b-by-d matrix\n",
    "#     yi: b-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     b: integer, the batch size\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "def mb_stochastic_objective_gradient(w, xi, yi, lam, b):\n",
    "    # Fill the function\n",
    "    # Follow the implementation of stochastic_objective_gradient\n",
    "    # Use matrix-vector multiplication; do not use FOR LOOP of vector-vector multiplications\n",
    "    obj = (1/b) * np.sum(np.log(1+np.exp(-yi*(xi@w)))) + (lam/2) * np.linalg.norm(w)**2\n",
    "    g = (1/b) * np.sum((-yi*(xi))/(1+np.exp(yi*(xi@w))), axis=0).reshape(w.shape) + lam * w\n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Implement mini-batch SGD\n",
    "\n",
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_stochastic_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     b: integer, the batch size\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     objvals: record of each iteration's objective value\n",
    "def mb_sgd(x, y, lam, b, stepsize, max_epoch=100, w=None):\n",
    "    # Fill the function\n",
    "    # Follow the implementation of sgd\n",
    "    # Record one objective value per epoch (not per iteration!)\n",
    "    if w == None:\n",
    "        w = np.zeros(x[0].shape).T\n",
    "    objvals = []\n",
    "    n = x.shape[0]\n",
    "    for _ in range(max_epoch):\n",
    "        permuted_inds = np.random.permutation(n)\n",
    "        x = x[permuted_inds]\n",
    "        y = y[permuted_inds]\n",
    "        epoch_objval = 0\n",
    "        for i in range(n//b):\n",
    "            xi = np.array(x[i*b:(i*b)+b])\n",
    "            yi = y[i*b:(i*b)+b]\n",
    "            obj, g = mb_stochastic_objective_gradient(w, xi, yi, lam, b)\n",
    "            w -= stepsize * g\n",
    "            epoch_objval += obj\n",
    "            objvals.append(obj)\n",
    "        print(epoch_objval/(n//b))\n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Run MB-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6499512553768725\n",
      "0.5883867107933716\n",
      "0.5520943707745225\n",
      "0.5288389175274769\n",
      "0.5129669507738004\n",
      "0.5015851700630255\n",
      "0.4929743392073068\n",
      "0.48637475922468865\n",
      "0.48119725925533274\n",
      "0.4770504591453695\n",
      "0.47362831349797485\n",
      "0.4709556980572409\n",
      "0.4685406501227901\n",
      "0.4667087798606141\n",
      "0.46510417790252784\n",
      "0.46375177023804887\n",
      "0.46265585697654243\n",
      "0.46162087141941716\n",
      "0.4608256435457706\n",
      "0.46008971794094977\n",
      "0.45948747688528\n",
      "0.4590122576317408\n",
      "0.4584328531576024\n",
      "0.45801923764931\n",
      "0.4577143970340846\n",
      "0.45739927080633064\n",
      "0.45711884337572856\n",
      "0.4568543917337533\n",
      "0.4566340060512995\n",
      "0.45648433719335024\n",
      "0.4563368656283201\n",
      "0.4561433919128996\n",
      "0.45610809395317553\n",
      "0.45596065313901957\n",
      "0.4558234602013041\n",
      "0.4557802693489905\n",
      "0.4557038791195782\n",
      "0.45551260963620477\n",
      "0.45551839775858854\n",
      "0.45547335180993\n",
      "0.4554556321995397\n",
      "0.455340446208372\n",
      "0.45534439762172435\n",
      "0.45528708237663673\n",
      "0.45534523116834613\n",
      "0.45524160096423955\n",
      "0.45517120137609657\n",
      "0.4552448394778182\n",
      "0.4552642312897257\n",
      "0.4552434767755221\n",
      "0.4551812788232555\n",
      "0.45525091523964545\n",
      "0.4551217393530501\n",
      "0.45518957346417954\n",
      "0.45516362471578437\n",
      "0.4551009734323582\n",
      "0.4550811331193553\n",
      "0.4551342682439678\n",
      "0.4550954159358347\n",
      "0.45513362753137876\n",
      "0.45522768554757975\n",
      "0.45513353462441647\n",
      "0.45500829242960705\n",
      "0.45510158382992316\n",
      "0.4550734319457915\n",
      "0.45506040706883233\n",
      "0.45512477809577667\n",
      "0.4551304213650772\n",
      "0.45500106483308633\n",
      "0.4550360122505116\n",
      "0.4551438510640188\n",
      "0.45506541897326924\n",
      "0.4550085290502797\n",
      "0.45504958594629963\n",
      "0.455087206546839\n",
      "0.4549812265948205\n",
      "0.4550306717317185\n",
      "0.4549889938187294\n",
      "0.4549855672553963\n",
      "0.4549538954064253\n",
      "0.4550649594726436\n",
      "0.45507605143743096\n",
      "0.45511738512544986\n",
      "0.45511862648790224\n",
      "0.45500872534769893\n",
      "0.4550364995905813\n",
      "0.4550978477327101\n",
      "0.4550220342955755\n",
      "0.45500902258453263\n",
      "0.45499586697313943\n",
      "0.45492496419881434\n",
      "0.4550619437087041\n",
      "0.45497874589940973\n",
      "0.45506274177722483\n",
      "0.4550167897521885\n",
      "0.4550169615960159\n",
      "0.4550645890715018\n",
      "0.45510589766767884\n",
      "0.45499465765938074\n",
      "0.4550667089373774\n"
     ]
    }
   ],
   "source": [
    "# MB-SGD with batch size b=8\n",
    "lam = 1E-6 # do not change\n",
    "b = 8 # do not change\n",
    "stepsize = 0.01 # you must tune this parameter\n",
    "\n",
    "w, objvals_mbsgd8 = mb_sgd(x_train, y_train, lam, b, stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6446106174142916\n",
      "0.5740163478936506\n",
      "0.5368829596061842\n",
      "0.5150560209356014\n",
      "0.5005368638167014\n",
      "0.4902320691310618\n",
      "0.48273991520871196\n",
      "0.4776581179992627\n",
      "0.47377132337286276\n",
      "0.4699265169445379\n",
      "0.4676261249811945\n",
      "0.4652652814794239\n",
      "0.4636995995668009\n",
      "0.4624302161757444\n",
      "0.46128640044050967\n",
      "0.46008315084248813\n",
      "0.4595659161422643\n",
      "0.45889702963537393\n",
      "0.45833858538739153\n",
      "0.4578378206322619\n",
      "0.45707644203803194\n",
      "0.45718485703523803\n",
      "0.4568274206691817\n",
      "0.4568674987661817\n",
      "0.45650287267255696\n",
      "0.4564789733702085\n",
      "0.4559782827111295\n",
      "0.45618583979400523\n",
      "0.45590838565469116\n",
      "0.45564846881996457\n",
      "0.45566957749356324\n",
      "0.45578777415398664\n",
      "0.45544442507305777\n",
      "0.4555880630121988\n",
      "0.45536872956936153\n",
      "0.45549223338240263\n",
      "0.4554533154361577\n",
      "0.45523780047280454\n",
      "0.4555818670665183\n",
      "0.4550065652632777\n",
      "0.455075988237018\n",
      "0.45544448009359806\n",
      "0.4551244955450602\n",
      "0.45499301807086356\n",
      "0.4555205454913874\n",
      "0.45552555026999053\n",
      "0.4552016612700312\n",
      "0.4554503596392071\n",
      "0.45527994639853164\n",
      "0.4553002209254274\n",
      "0.4552782115530579\n",
      "0.455258677763248\n",
      "0.4551982206928941\n",
      "0.45504864950881574\n",
      "0.45540953697272146\n",
      "0.45544258707006974\n",
      "0.45528604244033016\n",
      "0.4548677713986026\n",
      "0.45518342532202877\n",
      "0.4552590181327876\n",
      "0.4552217235185633\n",
      "0.4551498394996079\n",
      "0.45515505657597394\n",
      "0.45517258888702206\n",
      "0.45513937612287964\n",
      "0.4550096264415825\n",
      "0.4549680706894833\n",
      "0.4553934660790297\n",
      "0.45519164282206914\n",
      "0.4552272992649259\n",
      "0.4552253848460669\n",
      "0.4554741222156674\n",
      "0.4551993088152811\n",
      "0.45504674100400067\n",
      "0.45497336931966015\n",
      "0.4553835041531326\n",
      "0.4554058656176225\n",
      "0.4555685613019403\n",
      "0.45505956694455635\n",
      "0.4550797511432741\n",
      "0.4552105298089656\n",
      "0.45553077332646746\n",
      "0.4551975893608162\n",
      "0.45496341155163894\n",
      "0.455522295032816\n",
      "0.4554998637702294\n",
      "0.45554613142381584\n",
      "0.45515316342609247\n",
      "0.4550528606872712\n",
      "0.4551765143438516\n",
      "0.4555952422508237\n",
      "0.4553973280413812\n",
      "0.4551837420106709\n",
      "0.4553441333265087\n",
      "0.45504874719473765\n",
      "0.4553547718548086\n",
      "0.45520793071437965\n",
      "0.45532823878821144\n",
      "0.45492484344722933\n",
      "0.45510794489575906\n"
     ]
    }
   ],
   "source": [
    "# MB-SGD with batch size b=64\n",
    "lam = 1E-6 # do not change\n",
    "b = 64 # do not change\n",
    "stepsize = 0.1 # you must tune this parameter\n",
    "\n",
    "w, objvals_mbsgd64 = mb_sgd(x_train, y_train, lam, b, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot and compare GD, SGD, and MB-SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are required to compare the following algorithms:\n",
    "\n",
    "- Gradient descent (GD)\n",
    "\n",
    "- SGD\n",
    "\n",
    "- MB-SGD with b=8\n",
    "\n",
    "- MB-SGD with b=64\n",
    "\n",
    "Follow the code in Section 4 to plot ```objective function value``` against ```epochs```. There should be four curves in the plot; each curve corresponds to one algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Logistic regression with $\\ell_2$-norm regularization is a strongly convex optimization problem. All the algorithms will converge to the same solution. **In the end, the ``objective function value`` of the 4 algorithms will be the same. If not the same, your implementation must be wrong. Do NOT submit wrong code and wrong result!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 4 curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
